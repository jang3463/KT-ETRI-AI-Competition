{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "분야2_inference.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn2nbpDsnE1x",
        "outputId": "ae6671bb-ae5e-4f4b-d842-e11153dcb872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# 구글 colab을 사용해주시고 런타임 유형은 GPU로 부탁드립니다.\n",
        "# preprocess_date 함수 안 data_path 설정 부탁드립니다.\n",
        "# save_pred 함수 안 model_path 설정 부탁드립니다.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data_utils\n",
        "import os"
      ],
      "metadata": {
        "id": "NEv6HH2Bnbwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed(seed):\n",
        "    np.random.seed(seed) \n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "def seed_worker(_worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "\n",
        "def preprocess_data():\n",
        "    # 학습 데이터 읽기. 경로 설정에 주의 하세요!\n",
        "    path = '/content/drive/MyDrive/kt 이상탐지/data/Media'\n",
        "    file_list = os.listdir(path)\n",
        "\n",
        "    df = []\n",
        "    for file in file_list:\n",
        "      file_path = os.path.join(path, file)\n",
        "      data = pd.read_csv(file_path)\n",
        "      df.append(data)\n",
        "\n",
        "    # print(f'전체 데이터 세트. \\n{df}\\n')\n",
        "\n",
        "    # TODO: 예시코드 실행을 위한 Train_set/Test_set 분할입니다. 반드시 이 형태로 학습/테스트할 필요는 없습니다.\n",
        "    train_set = []\n",
        "    test_set = []\n",
        "    for data in df:\n",
        "      data['Timestamp'] = data['Timestamp'].apply(lambda x : x[:4]+'-'+x[4:6]+'-'+x[6:8]+' '+x[9:11]+':'+x[11:13]+':'+'00')\n",
        "      data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
        "      end_of_year = data.index[data['Timestamp'] == '2017-12-31 23:55:00'].tolist()[0]\n",
        "      train_set.append(data[:end_of_year+1])  # 2017 1.1 - 12.31 분리\n",
        "      test_set.append(data[end_of_year+1:])  # 2018 1.1 - 12.31 분리\n",
        "\n",
        "    # print(f'2017년. \\n{train_set}\\n')\n",
        "    # print(f'2018년. \\n{test_set}\\n')\n",
        "\n",
        "    # -----------------------------------\n",
        "    # TODO: 데이터 분석을 통해 다양한 전처리를 시도 해보세요!\n",
        "    for i in range(len(train_set)):\n",
        "      train = train_set[i]\n",
        "      test = test_set[i]\n",
        "      cn = len(train.columns)\n",
        "      cl = train.columns[1:]\n",
        "      n = 1\n",
        "      if cn == 13:\n",
        "        for j in range(1,cn//3+1):\n",
        "          train['menu_svr{}'.format(j)] = train[train.columns[n]] + train[train.columns[n+1]] + train[train.columns[n+2]]\n",
        "          test['menu_svr{}'.format(j)] = test[train.columns[n]] + test[train.columns[n+1]] + test[train.columns[n+2]]\n",
        "          n += 3\n",
        "        train.drop(columns = cl, inplace=True)\n",
        "        test.drop(columns = cl, inplace=True)\n",
        "        menu_train = train\n",
        "        menu_test = test\n",
        "      if cn == 16:\n",
        "        for j in range(1,cn//3+1):\n",
        "          train['login_svr{}'.format(j)] = train[train.columns[n]] + train[train.columns[n+1]] + train[train.columns[n+2]]\n",
        "          test['login_svr{}'.format(j)] = test[train.columns[n]] + test[train.columns[n+1]] + test[train.columns[n+2]]\n",
        "          n += 3\n",
        "        train.drop(columns = cl, inplace=True)\n",
        "        test.drop(columns = cl, inplace=True)\n",
        "        login_train = train\n",
        "        login_test = test\n",
        "      if cl[0] == 'INFO-01-Request':\n",
        "        train['info_svr1'] = train[train.columns[1]] + train[train.columns[2]] + train[train.columns[3]]\n",
        "        test['info_svr1'] = test[train.columns[1]] + test[train.columns[2]] + test[train.columns[3]]\n",
        "        train.drop(columns = cl, inplace=True)\n",
        "        test.drop(columns = cl, inplace=True)\n",
        "        info_train = train\n",
        "        info_test = test\n",
        "      if cl[0] == 'STREAM-01-Session':\n",
        "        stream_train = train\n",
        "        stream_test = test\n",
        "\n",
        "    preprocessed_train_set = pd.concat([menu_train.iloc[:,1:], login_train.iloc[:,1:],info_train.iloc[:,1:],stream_train.iloc[:,1:]],axis=1)\n",
        "    preprocessed_train_set.fillna(0, inplace =True)\n",
        "\n",
        "    preprocessed_test_set = pd.concat([menu_test.iloc[:,1:], login_test.iloc[:,1:],info_test.iloc[:,1:],stream_test.iloc[:,1:]],axis=1)\n",
        "\n",
        "    VALID_COLUMNS_IN_TRAIN_DATASET = preprocessed_train_set.columns\n",
        "\n",
        "    TAG_MIN = preprocessed_train_set[VALID_COLUMNS_IN_TRAIN_DATASET].min()\n",
        "    TAG_MAX = preprocessed_train_set[VALID_COLUMNS_IN_TRAIN_DATASET].max()\n",
        "\n",
        "    def normalize(df):\n",
        "      ndf = df.copy()\n",
        "      for c in df.columns:\n",
        "          if TAG_MIN[c] == TAG_MAX[c]:\n",
        "              ndf[c] = df[c] - TAG_MIN[c]\n",
        "          else:\n",
        "              ndf[c] = (df[c] - TAG_MIN[c]) / (TAG_MAX[c] - TAG_MIN[c])\n",
        "      return ndf\n",
        "\n",
        "    def boundary_check(df):\n",
        "        x = np.array(df, dtype=np.float32)\n",
        "        return np.any(x > 1.0), np.any(x < 0), np.any(np.isnan(x))\n",
        "\n",
        "    TRAIN_DF = normalize(preprocessed_train_set[VALID_COLUMNS_IN_TRAIN_DATASET]).ewm(alpha=0.9).mean()\n",
        "    TEST_DF = normalize(preprocessed_test_set[VALID_COLUMNS_IN_TRAIN_DATASET]).ewm(alpha=0.9).mean()\n",
        "\n",
        "    print(boundary_check(TRAIN_DF))\n",
        "    print(boundary_check(TEST_DF))\n",
        "\n",
        "    window_size=12\n",
        "\n",
        "    windows_train=TRAIN_DF.values[np.arange(window_size)[None, :] + np.arange(TRAIN_DF.shape[0]-window_size)[:, None]]\n",
        "\n",
        "    atest = np.arange(window_size)[None, :] + np.arange(TEST_DF.shape[0])[:, None]\n",
        "    windows_test=TEST_DF.values[np.where(atest > TEST_DF.shape[0]-1, TEST_DF.shape[0]-1, atest)]\n",
        "\n",
        "\n",
        "    return windows_train, windows_test\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_size, latent_size):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(in_size, int(in_size/2))\n",
        "    self.linear2 = nn.Linear(int(in_size/2), int(in_size/4))\n",
        "    self.linear3 = nn.Linear(int(in_size/4), latent_size)\n",
        "    self.relu = nn.ReLU(True)\n",
        "        \n",
        "  def forward(self, w):\n",
        "    out = self.linear1(w)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear3(out)\n",
        "    z = self.relu(out)\n",
        "    return z\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, latent_size, out_size):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(latent_size, int(out_size/4))\n",
        "    self.linear2 = nn.Linear(int(out_size/4), int(out_size/2))\n",
        "    self.linear3 = nn.Linear(int(out_size/2), out_size)\n",
        "    self.relu = nn.ReLU(True)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "  def forward(self, z):\n",
        "    out = self.linear1(z)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear3(out)\n",
        "    w = self.sigmoid(out)\n",
        "    return w\n",
        "    \n",
        "class UsadModel(nn.Module):\n",
        "  def __init__(self, w_size, z_size):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(w_size, z_size)\n",
        "    self.decoder1 = Decoder(z_size, w_size)\n",
        "    self.decoder2 = Decoder(z_size, w_size)\n",
        "  \n",
        "  def training_step(self, batch, n):\n",
        "    z = self.encoder(batch)\n",
        "    w1 = self.decoder1(z)\n",
        "    w2 = self.decoder2(z)\n",
        "    w3 = self.decoder2(self.encoder(w1))\n",
        "    loss1 = 1/n*torch.mean((batch-w1)**2)+(1-1/n)*torch.mean((batch-w3)**2)\n",
        "    loss2 = 1/n*torch.mean((batch-w2)**2)-(1-1/n)*torch.mean((batch-w3)**2)\n",
        "    return loss1,loss2\n",
        "\n",
        "  def validation_step(self, batch, n):\n",
        "    z = self.encoder(batch)\n",
        "    w1 = self.decoder1(z)\n",
        "    w2 = self.decoder2(z)\n",
        "    w3 = self.decoder2(self.encoder(w1))\n",
        "    loss1 = 1/n*torch.mean((batch-w1)**2)+(1-1/n)*torch.mean((batch-w3)**2)\n",
        "    loss2 = 1/n*torch.mean((batch-w2)**2)-(1-1/n)*torch.mean((batch-w3)**2)\n",
        "    return {'val_loss1': loss1, 'val_loss2': loss2}\n",
        "        \n",
        "  def validation_epoch_end(self, outputs):\n",
        "    batch_losses1 = [x['val_loss1'] for x in outputs]\n",
        "    epoch_loss1 = torch.stack(batch_losses1).mean()\n",
        "    batch_losses2 = [x['val_loss2'] for x in outputs]\n",
        "    epoch_loss2 = torch.stack(batch_losses2).mean()\n",
        "    return {'val_loss1': epoch_loss1.item(), 'val_loss2': epoch_loss2.item()}\n",
        "    \n",
        "  def epoch_end(self, epoch, result):\n",
        "    print(\"Epoch [{}], val_loss1: {:.4f}, val_loss2: {:.4f}\".format(epoch, result['val_loss1'], result['val_loss2']))\n",
        "    \n",
        "def testing(model, test_loader, alpha=.5, beta=.5):\n",
        "    results=[]\n",
        "    for [batch] in test_loader:\n",
        "        batch=to_device(batch,device)\n",
        "        w1=model.decoder1(model.encoder(batch))\n",
        "        w2=model.decoder2(model.encoder(w1))\n",
        "        results.append(alpha*torch.mean((batch-w1)**2,axis=1)+beta*torch.mean((batch-w2)**2,axis=1))\n",
        "    return results\n",
        "\n",
        "def to_device(data, device):\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def save_pred(test_data):\n",
        "    # TODO: 모델을 활용해, 2018년 전체에 대한 예측을 수행하세요!\n",
        "    \n",
        "    w_size=test_data.shape[1]*test_data.shape[2]\n",
        "    z_size=test_data.shape[1]*hidden_size\n",
        "\n",
        "    model_path = '/content/drive/MyDrive/kt 이상탐지/model/usad_model.pt'\n",
        "    model = UsadModel(w_size, z_size).cuda()\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
        "    torch.from_numpy(test_data).float().view(([test_data.shape[0],w_size]))\n",
        "    ) , batch_size=BATCH_SIZE, shuffle=False, num_workers=0, worker_init_fn=seed_worker)\n",
        "    \n",
        "    results=testing(model,test_loader)\n",
        "\n",
        "    anomalyscore = np.concatenate([torch.stack(results[:-1]).flatten().detach().cpu().numpy(),\n",
        "                              results[-1].flatten().detach().cpu().numpy()])\n",
        "    df_as = pd.DataFrame(anomalyscore)\n",
        "    \n",
        "    THRESHOLD = df_as[0].quantile(0.9985)\n",
        "    print(THRESHOLD)\n",
        "\n",
        "    pred = [1 if e > THRESHOLD else 0 for e in anomalyscore]\n",
        "    pred = np.array(pred)\n",
        "\n",
        "    \n",
        "    # 예측된 결과를 제출하기 위한 포맷팅\n",
        "    answer = pd.DataFrame(pred, columns=['Prediction'])\n",
        "    print(f'예측 결과. \\n{answer}\\n')  # TODO: 제출 전 row size \"105120\" 확인\n",
        "    print(answer['Prediction'].value_counts())\n",
        "    answer.to_csv('Media_answer.csv', index=False)  # 정답을 제출하기 위해 저장\n",
        "\n",
        "\n",
        "# TODO: 제출 파일은 2018년 1월 1일 00시 00분-05분 부터 2018년 12월 31일 23시 55분-00분 구간의 이상 이벤트를 예측한\n",
        "# .csv 형식으로 저장해야 합니다.\n",
        "# 예측 데이터프레임의 크기는 [105120 * 1]입니다.\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    seed(15)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    BATCH_SIZE =  256\n",
        "    N_EPOCHS = 50\n",
        "    hidden_size = 10\n",
        "\n",
        "    # 데이터 전처리\n",
        "    train_data, test_data = preprocess_data()\n",
        "\n",
        "    # 예측 결과 저장\n",
        "    save_pred(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etfgo9VknbtJ",
        "outputId": "1da13ec4-e46c-45dd-cef1-dcde4b1df6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(False, False, False)\n",
            "(True, False, False)\n",
            "0.1981596269905579\n",
            "예측 결과. \n",
            "        Prediction\n",
            "0                0\n",
            "1                0\n",
            "2                0\n",
            "3                0\n",
            "4                0\n",
            "...            ...\n",
            "105115           1\n",
            "105116           1\n",
            "105117           1\n",
            "105118           1\n",
            "105119           1\n",
            "\n",
            "[105120 rows x 1 columns]\n",
            "\n",
            "0    104962\n",
            "1       158\n",
            "Name: Prediction, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}