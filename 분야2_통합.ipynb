{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "분야2_통합.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV7g9vZJDrBd",
        "outputId": "7b6f9335-6bd2-4781-c3d8-7bc7d6ccd4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 구글 colab을 사용해주시고 런타임 유형은 GPU로 부탁드립니다.\n",
        "# preprocess_date함수 안쪽 data_path 설정 부탁드립니다.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data_utils\n",
        "import os"
      ],
      "metadata": {
        "id": "d_ANyyVPDuno"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed(seed):\n",
        "    np.random.seed(seed) \n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "def seed_worker(_worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "\n",
        "def preprocess_data():\n",
        "    # 학습 데이터 읽기. 경로 설정에 주의 하세요!\n",
        "    path = '/content/drive/MyDrive/kt 이상탐지/data/Media'\n",
        "    file_list = os.listdir(path)\n",
        "\n",
        "    df = []\n",
        "    for file in file_list:\n",
        "      file_path = os.path.join(path, file)\n",
        "      data = pd.read_csv(file_path)\n",
        "      df.append(data)\n",
        "\n",
        "    # print(f'전체 데이터 세트. \\n{df}\\n')\n",
        "\n",
        "    # TODO: 예시코드 실행을 위한 Train_set/Test_set 분할입니다. 반드시 이 형태로 학습/테스트할 필요는 없습니다.\n",
        "    train_set = []\n",
        "    test_set = []\n",
        "    for data in df:\n",
        "      data['Timestamp'] = data['Timestamp'].apply(lambda x : x[:4]+'-'+x[4:6]+'-'+x[6:8]+' '+x[9:11]+':'+x[11:13]+':'+'00')\n",
        "      data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
        "      end_of_year = data.index[data['Timestamp'] == '2017-12-31 23:55:00'].tolist()[0]\n",
        "      train_set.append(data[:end_of_year+1])  # 2017 1.1 - 12.31 분리\n",
        "      test_set.append(data[end_of_year+1:])  # 2018 1.1 - 12.31 분리\n",
        "\n",
        "    # print(f'2017년. \\n{train_set}\\n')\n",
        "    # print(f'2018년. \\n{test_set}\\n')\n",
        "\n",
        "    # -----------------------------------\n",
        "    # TODO: 데이터 분석을 통해 다양한 전처리를 시도 해보세요!\n",
        "    for i in range(len(train_set)):\n",
        "      train = train_set[i]\n",
        "      test = test_set[i]\n",
        "      cn = len(train.columns)\n",
        "      cl = train.columns[1:]\n",
        "      n = 1\n",
        "      if cn == 13:\n",
        "        for j in range(1,cn//3+1):\n",
        "          train['menu_svr{}'.format(j)] = train[train.columns[n]] + train[train.columns[n+1]] + train[train.columns[n+2]]\n",
        "          test['menu_svr{}'.format(j)] = test[train.columns[n]] + test[train.columns[n+1]] + test[train.columns[n+2]]\n",
        "          n += 3\n",
        "        train.drop(columns = cl, inplace=True)\n",
        "        test.drop(columns = cl, inplace=True)\n",
        "        menu_train = train\n",
        "        menu_test = test\n",
        "      if cn == 16:\n",
        "        for j in range(1,cn//3+1):\n",
        "          train['login_svr{}'.format(j)] = train[train.columns[n]] + train[train.columns[n+1]] + train[train.columns[n+2]]\n",
        "          test['login_svr{}'.format(j)] = test[train.columns[n]] + test[train.columns[n+1]] + test[train.columns[n+2]]\n",
        "          n += 3\n",
        "        train.drop(columns = cl, inplace=True)\n",
        "        test.drop(columns = cl, inplace=True)\n",
        "        login_train = train\n",
        "        login_test = test\n",
        "      if cl[0] == 'INFO-01-Request':\n",
        "        train['info_svr1'] = train[train.columns[1]] + train[train.columns[2]] + train[train.columns[3]]\n",
        "        test['info_svr1'] = test[train.columns[1]] + test[train.columns[2]] + test[train.columns[3]]\n",
        "        train.drop(columns = cl, inplace=True)\n",
        "        test.drop(columns = cl, inplace=True)\n",
        "        info_train = train\n",
        "        info_test = test\n",
        "      if cl[0] == 'STREAM-01-Session':\n",
        "        stream_train = train\n",
        "        stream_test = test\n",
        "\n",
        "    preprocessed_train_set = pd.concat([menu_train.iloc[:,1:], login_train.iloc[:,1:],info_train.iloc[:,1:],stream_train.iloc[:,1:]],axis=1)\n",
        "    preprocessed_train_set.fillna(0, inplace =True)\n",
        "\n",
        "    preprocessed_test_set = pd.concat([menu_test.iloc[:,1:], login_test.iloc[:,1:],info_test.iloc[:,1:],stream_test.iloc[:,1:]],axis=1)\n",
        "\n",
        "    VALID_COLUMNS_IN_TRAIN_DATASET = preprocessed_train_set.columns\n",
        "\n",
        "    TAG_MIN = preprocessed_train_set[VALID_COLUMNS_IN_TRAIN_DATASET].min()\n",
        "    TAG_MAX = preprocessed_train_set[VALID_COLUMNS_IN_TRAIN_DATASET].max()\n",
        "\n",
        "    def normalize(df):\n",
        "      ndf = df.copy()\n",
        "      for c in df.columns:\n",
        "          if TAG_MIN[c] == TAG_MAX[c]:\n",
        "              ndf[c] = df[c] - TAG_MIN[c]\n",
        "          else:\n",
        "              ndf[c] = (df[c] - TAG_MIN[c]) / (TAG_MAX[c] - TAG_MIN[c])\n",
        "      return ndf\n",
        "\n",
        "    def boundary_check(df):\n",
        "        x = np.array(df, dtype=np.float32)\n",
        "        return np.any(x > 1.0), np.any(x < 0), np.any(np.isnan(x))\n",
        "\n",
        "    TRAIN_DF = normalize(preprocessed_train_set[VALID_COLUMNS_IN_TRAIN_DATASET]).ewm(alpha=0.9).mean()\n",
        "    TEST_DF = normalize(preprocessed_test_set[VALID_COLUMNS_IN_TRAIN_DATASET]).ewm(alpha=0.9).mean()\n",
        "\n",
        "    print(boundary_check(TRAIN_DF))\n",
        "    print(boundary_check(TEST_DF))\n",
        "\n",
        "    window_size=12\n",
        "\n",
        "    windows_train=TRAIN_DF.values[np.arange(window_size)[None, :] + np.arange(TRAIN_DF.shape[0]-window_size)[:, None]]\n",
        "\n",
        "    atest = np.arange(window_size)[None, :] + np.arange(TEST_DF.shape[0])[:, None]\n",
        "    windows_test=TEST_DF.values[np.where(atest > TEST_DF.shape[0]-1, TEST_DF.shape[0]-1, atest)]\n",
        "\n",
        "\n",
        "    return windows_train, windows_test\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_size, latent_size):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(in_size, int(in_size/2))\n",
        "    self.linear2 = nn.Linear(int(in_size/2), int(in_size/4))\n",
        "    self.linear3 = nn.Linear(int(in_size/4), latent_size)\n",
        "    self.relu = nn.ReLU(True)\n",
        "        \n",
        "  def forward(self, w):\n",
        "    out = self.linear1(w)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear3(out)\n",
        "    z = self.relu(out)\n",
        "    return z\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, latent_size, out_size):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(latent_size, int(out_size/4))\n",
        "    self.linear2 = nn.Linear(int(out_size/4), int(out_size/2))\n",
        "    self.linear3 = nn.Linear(int(out_size/2), out_size)\n",
        "    self.relu = nn.ReLU(True)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "  def forward(self, z):\n",
        "    out = self.linear1(z)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear3(out)\n",
        "    w = self.sigmoid(out)\n",
        "    return w\n",
        "    \n",
        "class UsadModel(nn.Module):\n",
        "  def __init__(self, w_size, z_size):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(w_size, z_size)\n",
        "    self.decoder1 = Decoder(z_size, w_size)\n",
        "    self.decoder2 = Decoder(z_size, w_size)\n",
        "  \n",
        "  def training_step(self, batch, n):\n",
        "    z = self.encoder(batch)\n",
        "    w1 = self.decoder1(z)\n",
        "    w2 = self.decoder2(z)\n",
        "    w3 = self.decoder2(self.encoder(w1))\n",
        "    loss1 = 1/n*torch.mean((batch-w1)**2)+(1-1/n)*torch.mean((batch-w3)**2)\n",
        "    loss2 = 1/n*torch.mean((batch-w2)**2)-(1-1/n)*torch.mean((batch-w3)**2)\n",
        "    return loss1,loss2\n",
        "\n",
        "  def validation_step(self, batch, n):\n",
        "    z = self.encoder(batch)\n",
        "    w1 = self.decoder1(z)\n",
        "    w2 = self.decoder2(z)\n",
        "    w3 = self.decoder2(self.encoder(w1))\n",
        "    loss1 = 1/n*torch.mean((batch-w1)**2)+(1-1/n)*torch.mean((batch-w3)**2)\n",
        "    loss2 = 1/n*torch.mean((batch-w2)**2)-(1-1/n)*torch.mean((batch-w3)**2)\n",
        "    return {'val_loss1': loss1, 'val_loss2': loss2}\n",
        "        \n",
        "  def validation_epoch_end(self, outputs):\n",
        "    batch_losses1 = [x['val_loss1'] for x in outputs]\n",
        "    epoch_loss1 = torch.stack(batch_losses1).mean()\n",
        "    batch_losses2 = [x['val_loss2'] for x in outputs]\n",
        "    epoch_loss2 = torch.stack(batch_losses2).mean()\n",
        "    return {'val_loss1': epoch_loss1.item(), 'val_loss2': epoch_loss2.item()}\n",
        "    \n",
        "  def epoch_end(self, epoch, result):\n",
        "    print(\"Epoch [{}], val_loss1: {:.4f}, val_loss2: {:.4f}\".format(epoch, result['val_loss1'], result['val_loss2']))\n",
        "    \n",
        "def evaluate(model, val_loader, n):\n",
        "    outputs = [model.validation_step(to_device(batch,device), n) for [batch] in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def training(epochs, model, train_loader, val_loader, opt_func=torch.optim.Adam):\n",
        "    history = []\n",
        "    optimizer1 = opt_func(list(model.encoder.parameters())+list(model.decoder1.parameters()))\n",
        "    optimizer2 = opt_func(list(model.encoder.parameters())+list(model.decoder2.parameters()))\n",
        "    for epoch in range(epochs):\n",
        "        for [batch] in train_loader:\n",
        "            batch=to_device(batch,device)\n",
        "            \n",
        "            #Train AE1\n",
        "            loss1,loss2 = model.training_step(batch,epoch+1)\n",
        "            loss1.backward()\n",
        "            optimizer1.step()\n",
        "            optimizer1.zero_grad()\n",
        "            \n",
        "            \n",
        "            #Train AE2\n",
        "            loss1,loss2 = model.training_step(batch,epoch+1)\n",
        "            loss2.backward()\n",
        "            optimizer2.step()\n",
        "            optimizer2.zero_grad()\n",
        "            \n",
        "            \n",
        "        result = evaluate(model, val_loader, epoch+1)\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history\n",
        "    \n",
        "def testing(model, test_loader, alpha=.5, beta=.5):\n",
        "    results=[]\n",
        "    for [batch] in test_loader:\n",
        "        batch=to_device(batch,device)\n",
        "        w1=model.decoder1(model.encoder(batch))\n",
        "        w2=model.decoder2(model.encoder(w1))\n",
        "        results.append(alpha*torch.mean((batch-w1)**2,axis=1)+beta*torch.mean((batch-w2)**2,axis=1))\n",
        "    return results\n",
        "\n",
        "def to_device(data, device):\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------\n",
        "def train_model(train_data):\n",
        "    # TODO: 정상(0)과 이상(1)을 판단하기 위한 모델을 학습하세요!\n",
        "    w_size=train_data.shape[1]*train_data.shape[2]\n",
        "    z_size=train_data.shape[1]*hidden_size\n",
        "\n",
        "    train_data_train = train_data[:int(np.floor(.8 *  train_data.shape[0]))]\n",
        "    train_data_val = train_data[int(np.floor(.8 *  train_data.shape[0])):int(np.floor(train_data.shape[0]))]\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
        "        torch.from_numpy(train_data_train).float().view(([train_data_train.shape[0],w_size]))\n",
        "    ) , batch_size=BATCH_SIZE, shuffle=False, num_workers=0, worker_init_fn=seed_worker)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
        "        torch.from_numpy(train_data_val).float().view(([train_data_val.shape[0],w_size]))\n",
        "    ) , batch_size=BATCH_SIZE, shuffle=False, num_workers=0, worker_init_fn=seed_worker)    \n",
        "\n",
        "    model = UsadModel(w_size, z_size)\n",
        "    model = to_device(model, device)\n",
        "    history = training(N_EPOCHS, model, train_loader, val_loader)\n",
        "\n",
        "    torch.save(model.state_dict(), 'USAD_model_분야2.pt')\n",
        "    return model\n",
        "\n",
        "\n",
        "def save_pred(model, test_data):\n",
        "    # TODO: 모델을 활용해, 2018년 전체에 대한 예측을 수행하세요!\n",
        "    \n",
        "    w_size=test_data.shape[1]*test_data.shape[2]\n",
        "    z_size=test_data.shape[1]*hidden_size\n",
        "\n",
        "    # model = UsadModel(w_size, z_size).cuda()\n",
        "    # model.load_state_dict(torch.load('/content/usad_model.pt'))\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
        "    torch.from_numpy(test_data).float().view(([test_data.shape[0],w_size]))\n",
        "    ) , batch_size=BATCH_SIZE, shuffle=False, num_workers=0, worker_init_fn=seed_worker)\n",
        "    \n",
        "    results=testing(model,test_loader)\n",
        "\n",
        "    anomalyscore = np.concatenate([torch.stack(results[:-1]).flatten().detach().cpu().numpy(),\n",
        "                              results[-1].flatten().detach().cpu().numpy()])\n",
        "    df_as = pd.DataFrame(anomalyscore)\n",
        "    \n",
        "    THRESHOLD = df_as[0].quantile(0.9985)\n",
        "    print(THRESHOLD)\n",
        "\n",
        "    pred = [1 if e > THRESHOLD else 0 for e in anomalyscore]\n",
        "    pred = np.array(pred)\n",
        "\n",
        "    \n",
        "    # 예측된 결과를 제출하기 위한 포맷팅\n",
        "    answer = pd.DataFrame(pred, columns=['Prediction'])\n",
        "    print(f'예측 결과. \\n{answer}\\n')  # TODO: 제출 전 row size \"105120\" 확인\n",
        "    print(answer['Prediction'].value_counts())\n",
        "    answer.to_csv('Media_answer.csv', index=False)  # 정답을 제출하기 위해 저장\n",
        "\n",
        "\n",
        "# TODO: 제출 파일은 2018년 1월 1일 00시 00분-05분 부터 2018년 12월 31일 23시 55분-00분 구간의 이상 이벤트를 예측한\n",
        "# .csv 형식으로 저장해야 합니다.\n",
        "# 예측 데이터프레임의 크기는 [105120 * 1]입니다.\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    seed(15)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    BATCH_SIZE =  256\n",
        "    N_EPOCHS = 50\n",
        "    hidden_size = 10\n",
        "\n",
        "    # 데이터 전처리\n",
        "    train_data, test_data = preprocess_data()\n",
        "\n",
        "    # 모델 학습\n",
        "    model = train_model(train_data)\n",
        "\n",
        "    # 예측 결과 저장\n",
        "    save_pred(model, test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRTY75M9Dulv",
        "outputId": "896f3c98-f0e7-4f0d-a7f6-6e8fcfd8d926"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(False, False, False)\n",
            "(True, False, False)\n",
            "Epoch [0], val_loss1: 0.0019, val_loss2: 0.0018\n",
            "Epoch [1], val_loss1: 0.0032, val_loss2: -0.0007\n",
            "Epoch [2], val_loss1: 0.0592, val_loss2: -0.0565\n",
            "Epoch [3], val_loss1: 0.0073, val_loss2: -0.0057\n",
            "Epoch [4], val_loss1: 0.0707, val_loss2: -0.0689\n",
            "Epoch [5], val_loss1: 0.0734, val_loss2: -0.0719\n",
            "Epoch [6], val_loss1: 0.0754, val_loss2: -0.0742\n",
            "Epoch [7], val_loss1: 0.0768, val_loss2: -0.0758\n",
            "Epoch [8], val_loss1: 0.0093, val_loss2: -0.0080\n",
            "Epoch [9], val_loss1: 0.0077, val_loss2: -0.0067\n",
            "Epoch [10], val_loss1: 0.0082, val_loss2: -0.0074\n",
            "Epoch [11], val_loss1: 0.0059, val_loss2: -0.0053\n",
            "Epoch [12], val_loss1: 0.0067, val_loss2: -0.0061\n",
            "Epoch [13], val_loss1: 0.0815, val_loss2: -0.0804\n",
            "Epoch [14], val_loss1: 0.0070, val_loss2: -0.0064\n",
            "Epoch [15], val_loss1: 0.0069, val_loss2: -0.0064\n",
            "Epoch [16], val_loss1: 0.0078, val_loss2: -0.0073\n",
            "Epoch [17], val_loss1: 0.0079, val_loss2: -0.0075\n",
            "Epoch [18], val_loss1: 0.0072, val_loss2: -0.0068\n",
            "Epoch [19], val_loss1: 0.0079, val_loss2: -0.0075\n",
            "Epoch [20], val_loss1: 0.0433, val_loss2: -0.0411\n",
            "Epoch [21], val_loss1: 0.0069, val_loss2: -0.0066\n",
            "Epoch [22], val_loss1: 0.0086, val_loss2: -0.0082\n",
            "Epoch [23], val_loss1: 0.0085, val_loss2: -0.0081\n",
            "Epoch [24], val_loss1: 0.0087, val_loss2: -0.0084\n",
            "Epoch [25], val_loss1: 0.0087, val_loss2: -0.0084\n",
            "Epoch [26], val_loss1: 0.0091, val_loss2: -0.0088\n",
            "Epoch [27], val_loss1: 0.0092, val_loss2: -0.0089\n",
            "Epoch [28], val_loss1: 0.0095, val_loss2: -0.0092\n",
            "Epoch [29], val_loss1: 0.0088, val_loss2: -0.0085\n",
            "Epoch [30], val_loss1: 0.0093, val_loss2: -0.0090\n",
            "Epoch [31], val_loss1: 0.0093, val_loss2: -0.0090\n",
            "Epoch [32], val_loss1: 0.0097, val_loss2: -0.0094\n",
            "Epoch [33], val_loss1: 0.0103, val_loss2: -0.0100\n",
            "Epoch [34], val_loss1: 0.0103, val_loss2: -0.0100\n",
            "Epoch [35], val_loss1: 0.0466, val_loss2: -0.0456\n",
            "Epoch [36], val_loss1: 0.0804, val_loss2: -0.0794\n",
            "Epoch [37], val_loss1: 0.0802, val_loss2: -0.0795\n",
            "Epoch [38], val_loss1: 0.0857, val_loss2: -0.0844\n",
            "Epoch [39], val_loss1: 0.0850, val_loss2: -0.0846\n",
            "Epoch [40], val_loss1: 0.0114, val_loss2: -0.0108\n",
            "Epoch [41], val_loss1: 0.0108, val_loss2: -0.0104\n",
            "Epoch [42], val_loss1: 0.0125, val_loss2: -0.0121\n",
            "Epoch [43], val_loss1: 0.0103, val_loss2: -0.0100\n",
            "Epoch [44], val_loss1: 0.0100, val_loss2: -0.0098\n",
            "Epoch [45], val_loss1: 0.0098, val_loss2: -0.0096\n",
            "Epoch [46], val_loss1: 0.0114, val_loss2: -0.0112\n",
            "Epoch [47], val_loss1: 0.0102, val_loss2: -0.0100\n",
            "Epoch [48], val_loss1: 0.0125, val_loss2: -0.0123\n",
            "Epoch [49], val_loss1: 0.0110, val_loss2: -0.0108\n",
            "0.1981596269905579\n",
            "예측 결과. \n",
            "        Prediction\n",
            "0                0\n",
            "1                0\n",
            "2                0\n",
            "3                0\n",
            "4                0\n",
            "...            ...\n",
            "105115           1\n",
            "105116           1\n",
            "105117           1\n",
            "105118           1\n",
            "105119           1\n",
            "\n",
            "[105120 rows x 1 columns]\n",
            "\n",
            "0    104962\n",
            "1       158\n",
            "Name: Prediction, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}